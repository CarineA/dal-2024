{
  "hash": "db37fce32350321198060f92ccbc2955",
  "result": {
    "markdown": "---\ntitle: \"DAL tutorial - Week 6\"\nsubtitle: \"Data transformation II\"\neditor: visual\nformat: \n  html:\n    css: [webex.css]\n    include-after-body: [webex.js]\n---\n\n\n## Summary measures\n\n\n\n\n\nDuring the lecture, we have learnt two types of measures.\n\n::: callout-tip\n#### Summary measures\n\n**Measures of central tendency** (mean, median, mode) indicate the typical or central value of a sample.\n\n**Measures of dispersion** (min-max, range, standard deviation) indicate the dispersion of the sample values around the central tendency value.\n:::\n\nWhen you work with data, you always want to get summary measures for most of the variables in the data.\n\nData reports usually include summary measures. It is also important to understand which summary measure is appropriate for which type of variable.\n\nWe have covered this in the lecture, so we won't go over it again here. Instead, you will learn how to obtain summary measures using the `summarise()` function from the [dplyr](https://dplyr.tidyverse.org) tidyverse package.\n\n`summarise()` takes at least two arguments:\n\n-   The data frame to summarise.\n\n-   One or more summary functions.\n\nFor example, let's get the mean the reaction time column `RT`. Easy! (First attach the tidyverse and read the `song2020/shallow.csv` file into a variable called `shallow`.)\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarise(shallow, RT_mean = mean(RT))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"RT_mean\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"867.3592\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nGreat! The mean reaction times of the entire sample is 867.3592 ms.\n\nYou can round numbers with the `round()` function. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum <- 867.3592\nround(num)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 867\n```\n:::\n\n```{.r .cell-code}\nround(num, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 867.4\n```\n:::\n\n```{.r .cell-code}\nround(num, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 867.36\n```\n:::\n:::\n\n\nThe second argument sets the number of decimals to round to (by default, it is `0`, so the number is rounded to the nearest integer).\n\nLet's recalculate the mean by rounding it this time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarise(shallow, RT_mean = round(mean(RT)))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"RT_mean\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"867\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWhat if we want also the standard deviation? Easy: we use the `sd()` function. (Round the mean and SD with the `round()` function in your code).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# round the mean and SD\nsummarise(shallow, RT_mean = mean(RT), RT_sd = sd(RT))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"RT_mean\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"RT_sd\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"867.3592\",\"2\":\"292.9682\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nNow we know that reaction times are on average 867 ms long and have a standard deviation of about 293 ms (rounded to the nearest integer).\n\nLet's go all the way and also get the minimum and maximum RT values with the `min()` and `max()` functions (round all the summary measures).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarise(\n  shallow,\n  RT_mean = mean(RT), RT_sd = sd(RT),\n  RT_min = ..., RT_max = ...\n)\n```\n:::\n\n\nFab! When writing a data report, you could write something like this.\n\n> Reaction times are on average 867 ms long (SD = 293 ms), with values ranging from 0 to 1994 ms.\n\nWe won't go into the details of what standard deviations are, but you can just think of them as a relative measure of how dispersed the data are around the mean: the higher the SD, the greater the dispersion around the mean, i.e. the greater the variability in the data.\n\nWhen required, you can use the `median()` function to calculate the median, instead of the `mean()`. Go ahead and calculate the median reaction times in the data. Is it similar to the mean?\n\n### `NA`s\n\nMost base R functions behave unexpectedly if the vector they are used on contain `NA` values.\n\n`NA` is a special object in R, that indicates that a value is **N**ot **A**vailable, meaning that that observation does not have a value.\n\nFor example, in the following numeric vector, there are 5 objects:\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- c(3, 5, 3, NA, 4)\n```\n:::\n\n\nFour are numbers and one is `NA`.\n\nIf you calculate the mean of `a` with `mean()` something strange happens.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] NA\n```\n:::\n:::\n\n\nThe functions returns `NA`.\n\nThis is because by default when just one value in the vector is `NA` then operations on the vector will return `NA`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] NA\n```\n:::\n\n```{.r .cell-code}\nsum(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] NA\n```\n:::\n\n```{.r .cell-code}\nsd(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] NA\n```\n:::\n:::\n\n\nIf you want to discard the `NA` values when operating on a vector that contains them, you have to set the `na.rm` (for \"`NA` remove\") argument to `TRUE`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(a, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.75\n```\n:::\n\n```{.r .cell-code}\nsum(a, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15\n```\n:::\n\n```{.r .cell-code}\nsd(a, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9574271\n```\n:::\n:::\n\n\n::: callout-note\n#### Quiz 1\n\n\na. What does the `na.rm` argument of `mean()` do? <div class='webex-radiogroup' id='radio_ARYPKTGCVX'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ARYPKTGCVX\" value=\"\"></input> <span>It changes `NA`s to `FALSE`.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ARYPKTGCVX\" value=\"\"></input> <span>It converts `NA`s to `0`s.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ARYPKTGCVX\" value=\"answer\"></input> <span>It removes `NA`s before taking the mean.</span></label></div>\nb. Which is the mean of `c(4, 23, NA, 5)` when `na.rm` has the default value? <div class='webex-radiogroup' id='radio_DPYHUUWDQJ'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_DPYHUUWDQJ\" value=\"answer\"></input> <span>`NA`.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_DPYHUUWDQJ\" value=\"\"></input> <span>`0`.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_DPYHUUWDQJ\" value=\"\"></input> <span>`10.66`.</span></label></div>\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hint\n\nCheck the documentation of `?mean`.\n:::\n:::\n\nNote that R has a `mode()` function, but alas this is not the statistical mode. To get the mode of a categorical variable you can just count the occurrences of the values of that variable and the value that occurs the most is the mode!\n\nYou will learn how to count occurrences below. But first, let's see what density plots are!\n\n## Density plots\n\n::: callout-tip\n#### Density plots\n\n**Density plots** show the distribution (i.e. the \"probability density\") of the values of a continuous variable.\n\nThey are created with `geom_density()`.\n:::\n\nReaction times is a numeric continuous variable so density plots are appropriate.\n\nTo plot the probability density of a continuous variable, you can use the `density` geometry.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshallow |>\n  ggplot(aes(x = RT)) +\n  geom_density()\n```\n\n::: {.cell-output-display}\n![](tutorial-w06_files/figure-html/rt-dens-1.png){width=672}\n:::\n:::\n\n\nThe black solid curve in the plot indicates the **density** of the data (the *y*-axis) along the values of RT (the *x*-axis).\n\nThe higher the point of the curve is on the *y*-axis (i.e. the higher the density), the more data there is at the corresponding *x*-axis value.\n\nFor example, the highest point in the curve is at around 750 ms (the white vertical line between 500 and 1000 is 750).\n\nThis means that around 750 ms there are many observations.\n\nOn the other hand, if you look at the curve to the left of 500 ms RT and above 1500 ms RT, the height of the points forming the curve are much lower and in some cases they even go to 0 density (*y*-axis).\n\nNote that to create a density plot, you only need to specify the `x`-axis. The `y`-axis is the probability density, which is automatically calculated (a bit like counts in bar charts, remember?).\n\n### Make things cosy with a rug\n\nThe density line shows you a smoothed representation of the data distribution over RT values, but you might also want to see the raw data represented on the *x*sxis.\n\nYou can do so by adding the `rug` geometry. Go ahead and add a rug...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshallow |>\n  ggplot(aes(RT)) +\n  geom_density() +\n  ...\n```\n:::\n\n\nYou should get the following:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](tutorial-w06_files/figure-html/rt-dens-rug-1.png){width=672}\n:::\n:::\n\n\nNice huh? You can also change the opacity of the ticks of the rug to have a better sense of how many ticks there are at certain values on the *x*-axis.\n\nOpacity of geometries can be adjusted with the `alpha` argument: 0 means completely transparent and 1 means completely opaque.\n\nLet's set the alpha of the rug geometry to 0.1.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](tutorial-w06_files/figure-html/rt-dens-rug-2-1.png){width=672}\n:::\n:::\n\n\nCan you see how the blackest parts of the rug correspond to the higher parts of the density curve?\n\n::: callout-tip\n#### Rug\n\nRaw data can be shown with a **rug**, i.e. ticks on the axes that mark where the data is.\n\nYou can add a rug with `geom_rug()`.\n:::\n\n::: callout-note\n#### Quiz 2\n\nWhat can you notice about the distribution of RT values?\n\n::: {.callout-tip collapse=\"true\"}\n#### Hint\n\nIs the distribution symmetric around the highest density point?\n:::\n:::\n\nKeep reading to learn how to count occurrences.\n\n## Count occurrences\n\nOften, you need to count occurrences in the data frame based on the values of specific columns.\n\nFor example, let's count the number of correct and incorrect trials in the `shallow` data frame.\n\nThe column `ACC` tells us whether a trial is incorrect `0` or correct `1`. (We will see how this way of coding binary variables, with `0`s and `1`s is not an ideal, although very common way, of coding binary variables. For now let's keep it as is.)\n\nWe can use the `count()` function from the [dplyr](https://dplyr.tidyverse.org) tidyverse package to count the number of occurrences for each value of a specific column.\n\nThe function `count()` takes the name of a tibble and the name of the column you want to count values in.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount(shallow, ACC)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"ACC\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0\",\"2\":\"849\"},{\"1\":\"1\",\"2\":\"5651\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# You can also write that as\n# shallow |> count(ACC)\n```\n:::\n\n\nHow many *correct* trials are there in the `shallow` tibble? And how many *incorrect* trials?\n\nNote that you can add **multiple column names, separated by commas**, to get counts for the combinations of values of each column.\n\nTry to get counts of the combination of `ACC` and `Group` (`L1` vs `L2` participants). Replace `...` with the right code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount(shallow, ...)\n```\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hint\n\nIn `count()`, include the names of the two columns you want to get counts of, separated by commas.\n:::\n\nThis is the output:\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"ACC\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Group\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0\",\"2\":\"L1\",\"3\":\"330\"},{\"1\":\"0\",\"2\":\"L2\",\"3\":\"519\"},{\"1\":\"1\",\"2\":\"L1\",\"3\":\"2570\"},{\"1\":\"1\",\"2\":\"L2\",\"3\":\"3081\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nAre there differences in accuracy between the L1 and L2 group? It's difficult to say just by looking at those numbers, because the total number of trials for L1 and L2 participants is different.\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Group\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"L1\",\"2\":\"2900\"},{\"1\":\"L2\",\"2\":\"3600\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThere are more L2 trials than L1 trials in the data set.\n\nWhen the total number of observations is not the same in the groups we are trying to compare, we can calculate the **proportion** instead of the raw count.\n\nThis should ring a bell. Do you remember `position = \"fill\"` in bar charts? This is based on the same reasoning.\n\nWe can calculate the proportion of correct and incorrect trials using a chain of functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshallow |>\n  add_count(Group, name = \"tot\") |>\n  count(Group, ACC, tot) |>\n  mutate(\n    prop = round(n / tot, 2)\n  )\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Group\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ACC\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tot\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"prop\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"L1\",\"2\":\"0\",\"3\":\"2900\",\"4\":\"330\",\"5\":\"0.11\"},{\"1\":\"L1\",\"2\":\"1\",\"3\":\"2900\",\"4\":\"2570\",\"5\":\"0.89\"},{\"1\":\"L2\",\"2\":\"0\",\"3\":\"3600\",\"4\":\"519\",\"5\":\"0.14\"},{\"1\":\"L2\",\"2\":\"1\",\"3\":\"3600\",\"4\":\"3081\",\"5\":\"0.86\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nTo learn what each line does, you can split the chain in multiple steps and inspect each step.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshallow_tot <- shallow |>\n  add_count(Group, name = \"tot\")\n\nshallow_count <- shallow_tot |>\n  count(Group, ACC, tot) \n\nshallow_prop <- shallow_count |>\n  mutate(\n    # round proportion to 2 decimals.\n    prop = round(n / tot, 2)\n  )\n```\n:::\n\n\nNow check `shallow_tot`, `shallow_count` and `shallow_prop`.\n\nBased on the proportion of correct trials in the L1 and L2 group, are there substantial differences in how the two groups performed? Or are they similar? If not, who was better?\n\n## Grouping data\n\nSometimes you might want to get summary measures for one column depending on different values of another column.\n\nYou can use the `group_by()` function from the [dplyr](https://dplyr.tidyverse.org) tidyverse package, together with `summarise()` to achieve that. Let's see how it works.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngroup_by(shallow, Group) |>\n  summarise(\n    RT_mean = round(mean(RT)),\n    RT_sd = round(sd(RT))\n  )\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Group\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"RT_mean\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"RT_sd\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"L1\",\"2\":\"789\",\"3\":\"239\"},{\"1\":\"L2\",\"2\":\"930\",\"3\":\"317\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThe `group_by()` function takes at least two arguments:\n\n-   The name of the tibble to group.\n\n-   The name of the columns to group the tibble by, separated by commas.\n\nHere we are grouping `shallow` by `Group`.\n\nIn fact, you can even use a pipe for the tibble of `group_by()` as we have done for other functions, like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshallow |>\n  group_by(Group) |>\n    summarise(\n      RT_mean = round(mean(RT)),\n      RT_sd = round(sd(RT))\n    )\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Group\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"RT_mean\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"RT_sd\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"L1\",\"2\":\"789\",\"3\":\"239\"},{\"1\":\"L2\",\"2\":\"930\",\"3\":\"317\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n::: callout-note\n#### Quiz 3\n\n\nWhich of the following returns the number of words in `shallow`? <div class='webex-radiogroup' id='radio_CXINHZEJHJ'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_CXINHZEJHJ\" value=\"\"></input> <span>`count(shallow, Target)`.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_CXINHZEJHJ\" value=\"answer\"></input> <span>`shallow |> distinct(Target) |> count()`.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_CXINHZEJHJ\" value=\"\"></input> <span>`shallow |> count(Target)`.</span></label></div>\n\n:::\n\n## Log-transformation\n\nLook again at the density plot of reaction times. Can you see the long tail of the density line on the right side of it?\n\n**Reaction times are numeric and continuous, but can only be positive!** Because of this, usually the distribution of reaction times looks like the one in plot: a big lump on the left side and a long tail to the right.\n\nIt is common practice to transform variables that can only tale positive values to reduce the asymmetry in the distribution.\n\nA common transformation is to calculate the **logarithm** of the values. You can calculate the logarithm (or log) of a number in R using the `log()` function. Calculating the logs of a variable is known as a **log-transformation**.\n\n::: callout-tip\n#### Log-transformation\n\nYou **log-transform** values by taking the logarithm (or log) of the values with the `log()` function.\n:::\n\nLet's log-transform the reaction times and plot them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshallow |>\n  ggplot(aes(x = log(RT))) +\n  geom_density(fill = \"#800000\", alpha = 0.7) +\n  geom_rug(alpha = 0.1)\n```\n\n::: {.cell-output-display}\n![](tutorial-w06_files/figure-html/fig-rt-dens-1.png){#fig-rt-dens width=672}\n:::\n:::\n\n\n::: {.callout-important collapse=\"true\"}\n#### Probability distributions\n\nA **probability distribution** defines the distribution of probabilities along a range of values. Probability distributions can be visualised using density lines. There are different families of probability distributions, each with its own characteristic shape. Probability distributions can be summarised with a set of parameters (each distribution family has its own parameters).\n\nAn important probability distribution family is the **Gaussian distribution** (aka normal distribution). In practice, data generated by a Gaussian distribution are very rare in the world, and most linguistic data follows other distributions.\n\nA Gaussian distribution can be defined by specifying the value of a mean and a standard deviation. Check out this [web app](https://seeing-theory.brown.edu/index.html) to learn more about probability and probability distributions.\n\nLog-transformation is normally applied to variables that follow the **log-normal** distribution. Logging a log-normal variable changes its properties so that it matches more the properties of a Gaussian distribution (that's why it's called log-normal: it becomes more Gaussian (aka normal) if you log it).\n\nProbability distributions are a very important concept for statistical modelling. Linear models are a very flexible tool to model all sorts of data with all sorts of distributions. If you want to learn more about linear models, check Bodo Winter's book, *Statistics for Linguistics with R*.\n:::\n\n## Practice\n\n::: callout-note\n#### Practice 1\n\n-   Read the `cameron2020/gestures.csv` file in R.\n\n-   Calculate the following:\n\n    -   Measure of central tendency and dispersion for the `count` column (it contains the number of gestures performed by each child in different tasks).\n\n    -   Measure of central tendency and dispersion for the `count` column grouped by `month` (the child's age).\n\n    -   Total number of gestures by children (`dyad`).\n\n    -   Number of children by `background`.\n\n-   Write a short paragraph where you report the measures.\n\n::: {.callout-tip collapse=\"true\"}\n#### Hint\n\n-   To calculate the total number of gestures by children, you need the `sum()` function.\n\n-   To calculate the number of children by background, you need the `distinct()` function.\n:::\n\n::: {.callout-warning collapse=\"true\"}\n#### Solution\n\nHave you tried doing the exercise and couldn't work it out?\n\nThe you can check the code solution here...\n\n::: {.callout-important collapse=\"true\"}\n#### Code\n\n``` r\ngestures |>\n  summarise(\n    count_med = median(count, na.rm = TRUE),\n    count_min = min(count, na.rm = TRUE),\n    count_max = max(count, na.rm = TRUE),\n    count_range = count_max - count_min\n  )\n\ngestures |>\n  group_by(months) |>\n  summarise(\n    count_med = median(count, na.rm = TRUE),\n    count_min = min(count, na.rm = TRUE),\n    count_max = max(count, na.rm = TRUE),\n    count_range = count_max - count_min\n  )\n\ngestures |>\n  group_by(dyad) |>\n  summarise(\n    count_tot = sum(count)\n  )\n\ngestures |>\n  distinct(background, dyad) |>\n  count(background)\n```\n:::\n:::\n:::\n\n## Summary\n\n::: {.callout-note appearance=\"minimal\"}\n**Data summaries**\n\n-   `summarise()` allows you to calculate measures of central tendency and dispersion (with `mean()`, `median()`, `min()` and `max()`, `sd()`, ...).\n\n-   `count()` lets you count the number of occurrences of levels in a categorical variable.\n\n-   `group_by()` allows you to group a tibble according to one or more variables.\n\n**Plotting**\n\n-   `geom_density()` creates density plots of continuous variables.\n\n-   `geom_rug()` adds raw data as ticks on the *x*-axis of density plots.\n:::\n",
    "supporting": [
      "tutorial-w06_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}